import requests
from bs4 import BeautifulSoup
import string
import os
from furl import furl

HEADERS = {"Accept-Language": "en-US,en;q=0.5"}
type_of_articles = [
    'Book Review', 'Nature Careers Podcast', 'Matters Arising', 'Career Column', 'Comment', 'Nature Podcast',
    'Research Highlight', 'Outline', 'Nature Briefing', 'Futures', 'News Explainer', 'Essay', 'News', 'Where I Work',
    'World View', 'News Feature', 'Correspondence', 'Outlook', 'Career Feature', 'News & Views']


def clean_filename(title):
    # Remove punctuation and replace spaces with underscores
    title = title.strip()
    title = title.translate(str.maketrans("", "", string.punctuation))
    title = title.replace(" ", "_")
    return title + ".txt"


def scrape_and_save_article(url):
    response = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.content, "html.parser")

    # Get article body
    article_body = soup.find("p", {"class": "article__teaser"})
    content = article_body.text.strip()

    return content


def main():
    # print("Scraping <type T> articles from Nature page <page_num> of 2020...")
    page_nums: int = int(input())
    article_type: str = input()

    for num in range(1, page_nums + 1):
        folder_name = f"Page_{num}"
        os.makedirs(folder_name, exist_ok=True)

        # Get articles of specific type from specific page
        url = furl("https://www.nature.com/nature/articles?sort=PubDate&year=2020").add({"page": num})
        response = requests.get(url, headers=HEADERS)
        soup = BeautifulSoup(response.content, "html.parser")
        articles = soup.find_all("article")
        # print(articles[0].prettify()) # Shows structure of an article item in HTML

        # Get articles` titles and full url links
        new_links = []
        new_titles = []
        for article in articles:
            type_tag = article.find("span", {"data-test": "article.type"})
            # print(type_tag.text) # Article type is written here
            if type_tag and type_tag.text.strip().lower() == article_type.lower():
                link_tag = article.find("a", {"data-track-action": "view article"})  # Relative href is written here
                link_title = link_tag.text.strip()  # Title of the article is written here
                if link_tag and link_tag.get("href"):
                    full_url = "https://www.nature.com" + link_tag["href"]  # Full link combines domain + relative links
                    # print(full_url)
                    new_titles.append(link_title)  # List with <type T> articles titles
                    new_links.append(full_url)  # List with <type T> articles full links

        # Output <type T> articles
        for title, url in zip(new_titles, new_links):
            print(title, url, sep=" : ")

        for title, url in zip(new_titles, new_links):
            try:
                filename = clean_filename(title)
                # print(filename) # Output filenames to be saved

                content = scrape_and_save_article(url)

                # Save to file
                file_path = os.path.join(folder_name, filename)
                with open(file_path, "w", encoding="utf-8") as file:
                    file.write(content)

            except Exception as e:
                print(f"Failed to process {url}: {e}")

    print("Saved all articles.")


if __name__ == '__main__':
    main()