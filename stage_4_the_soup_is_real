import requests
from bs4 import BeautifulSoup
import string

URL = "https://www.nature.com/nature/articles?sort=PubDate&year=2020&page=3"
BASE_URL = "https://www.nature.com"
HEADERS = {"Accept-Language": "en-US,en;q=0.5"}


def clean_filename(title):
    # Remove punctuation and replace spaces with underscores
    title = title.strip()
    title = title.translate(str.maketrans("", "", string.punctuation))
    title = title.replace(" ", "_")
    return title + ".txt"


def scrape_and_save_article(url):
    response = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(response.content, "html.parser")

    # Get article body
    # article_body = soup.find("div", {"class": "c-article-teaser-text"})
    article_body = soup.find("p", {"class": "article__teaser"})

    # if not article_body:
    #     return None

    # paragraphs = article_body.find_all("p")
    # content = "\n".join(p.text.strip() for p in paragraphs if p.text.strip())
    content = "\n"+ article_body.text.strip()

    # if not content:
    #     return None

    return content


def main():
    print("Scraping News articles from Nature page 3 of 2020...")

    response = requests.get(URL, headers=HEADERS)
    soup = BeautifulSoup(response.content, "html.parser")
    articles = soup.find_all("article")
    # print(articles[0].prettify()) # Shows structure of an article item in HTML

    news_links = []
    news_titles = []
    for article in articles:
        article_type = article.find("span", {"data-test": "article.type"})
        # print(article_type.text) # Article type is written here
        if article_type and article_type.text.strip().lower() == "news":
            link_tag = article.find("a", {"data-track-action": "view article"})  # Relative href is written here
            link_title = link_tag.text.strip() # Title of the article is written here
            if link_tag and link_tag.get("href"):
                full_url = BASE_URL + link_tag["href"] # Full link combines domain + relative links
                # print(full_url)
                news_titles.append(link_title)  # List with "News" type articles titles
                news_links.append(full_url)  # List with "News" type articles full links

    # # Output "News" type articles
    # for title, url in zip(news_titles, news_links):
    #     print(title, url, sep=" : ") # Output "News" type articles

    saved_files = []

    for title, url in zip(news_titles, news_links):
        try:
            filename = clean_filename(title)
            # print(filename) # Output filenames to be saved

            content = scrape_and_save_article(url)

            # Save to file
            with open(filename, "w", encoding="utf-8") as file:
                file.write(content)

            if filename:
                saved_files.append(filename)
        except Exception as e:
            print(f"Failed to process {url}: {e}")

    print("\nSaved articles:", saved_files)


if __name__ == '__main__':
    main()